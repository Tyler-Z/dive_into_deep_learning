{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6adddcaf",
   "metadata": {},
   "source": [
    "## 回归 vs 分类\n",
    "\n",
    "<img src=\"./pic/回归vs分类.PNG\"  width=500  heigth=500>\n",
    "<img src=\"./pic/回归vs分类2.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 从回归到多类分类 - 均方损失\n",
    "<img src=\"./pic/从回归到多类分类.PNG\"  width=500  heigth=500>\n",
    "\n",
    "\n",
    "## 从回归到多类分类 - 校验比例\n",
    "\n",
    "- 将操作子sofrmax，作用到所有的o上面得到y<sup>^</sup>\n",
    "- y是一个长为n的向量，里面每个元素都非负，且和为1\n",
    "- y^<sub>i</sub>是概率。比较两个概率的区别，作为损失\n",
    "- y是真实的概率，y^是预测的概率\n",
    "<img src=\"./pic/校验比例.PNG\"  width=500  heigth=500>\n",
    "\n",
    "\n",
    "## Softmax和交叉熵损失\n",
    "- p,q为两个概率【y vs y<sup>^</sup>】\n",
    "- y^<sub>y</sub> 为真实类别y的预测值y^，然后求log和负数\n",
    "- 对于分类问题，不关心对于非正确类的预测值，只关心正确类的预测值的置信度大小\n",
    "<img src=\"./pic/交叉熵.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## Softmax 总结\n",
    "<img src=\"./pic/softmax总结.PNG\"  width=500  heigth=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf07ae2",
   "metadata": {},
   "source": [
    "# 损失函数\n",
    "\n",
    "## 均方损失（L2 Loss）（对应的是高斯分布）\n",
    "- 优点：当预测值离0点（原点）比较近时，使用越来越小的梯度值，更新参数\n",
    "- 缺点：当离原点比较远时，我们并不想使用大的梯度值，更新参数\n",
    "<img src=\"./pic/L2 Loss.PNG\"  width=500  heigth=500>\n",
    "<img src=\"./pic/L2 Loss plot.PNG\"  width=500  heigth=500>\n",
    "- 蓝色线为损失函数\n",
    "- 橙色线为损失函数的导数\n",
    "- 绿色线为似然函数\n",
    "    - 最小化损失等价于最大化的似然函数\n",
    "    - 在给定数据的情况下，一个模型（这里模型就是权重），出现的概率有多大\n",
    "    - 就是拟合出来的参数（权重）最接近真实值\n",
    "\n",
    "\n",
    "## 线性损失（L1 Loss）（对应的是拉普拉斯分布）\n",
    "- 优点：不管预测值离0点有多远，梯度永远是常数\n",
    "- 缺点：0点处不可导，0点处的不平滑性。导致优化末期，优化不稳定\n",
    "<img src=\"./pic/L1 Loss.PNG\"  width=500  heigth=500>\n",
    "<img src=\"./pic/L1 Loss plot.PNG\"  width=500  heigth=500>\n",
    "\n",
    "\n",
    "## Huber's Robust Loss\n",
    "- 当真实值与预测值比较接近的时候，使用平方误差；其他情况使用线性误差。（结合L1和L2 loss）\n",
    "- 优点是能增强平方误差损失函数(MSE)对**离群点**的鲁棒性，用于回归问题。δ是一个可以自己设置的参数（δ=1）。\n",
    "- 当预测偏差小于 δ 时，它采用平方误差,\n",
    "- 当预测偏差大于 δ 时，采用的线性误差\n",
    "<img src=\"./pic/Huber Loss.PNG\"  width=500  heigth=500>\n",
    "<img src=\"./pic/Huber Loss plot.PNG\"  width=500  heigth=500>\n",
    "<img src=\"./pic/Huber Loss plot 2.PNG\"  width=500  heigth=500>\n",
    "- 在[-1, 1]之间，是一条曲线。在此之外是一条平滑的二次函数\n",
    "<img src=\"./pic/huber+L2 loss.PNG\"  width=500  heigth=500>\n",
    "- 上图绿色部分为huber loss\n",
    "- 紫色部分为L2loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa5d41",
   "metadata": {},
   "source": [
    "# 另外L1和L2范数常用于正则化项：\n",
    "\n",
    "- L1正则会制造稀疏的特征，大部分无用特征的权重会被置为0，有特征选择作用；\n",
    "- L2正则会让特征的权重不过大，使得特征的权重比较平均。\n",
    "<img src=\"./pic/L1 L2 norm.PNG\"  width=500  heigth=500>\n",
    "- 上图为1范数和2范数的图像，可以看出L1正则倾向于选择坐标轴上的参数（即出现0为稀疏解），L2正则倾向于选择均匀参数；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b99ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
