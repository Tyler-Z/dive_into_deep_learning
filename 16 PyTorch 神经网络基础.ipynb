{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb93dcf",
   "metadata": {},
   "source": [
    "- 自定义神经网络 + 访问参数 + 自定义层 + 读写文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d82f0",
   "metadata": {},
   "source": [
    "# 1. 层和块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8c2240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1587,  0.1611,  0.0056,  0.4666, -0.0986, -0.0558,  0.2867, -0.0337,\n",
       "         -0.1056, -0.1919],\n",
       "        [-0.1198,  0.3089,  0.0037,  0.3942, -0.1108, -0.0517,  0.1984, -0.0118,\n",
       "         -0.0220, -0.1338]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# nn.Linear有默认初始化\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)    # 2是批量大小，20是输入维度，所以net线性层数输入为20\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba18585",
   "metadata": {},
   "source": [
    "### nn.Sequential定义了一种特殊的Module。在Pytorch里，module是一个很重要的概念\n",
    "\n",
    "- 任何的层与神经网络都是module的一个子类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc601b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义块（自定义mlp实现上面的模型）\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        super().__init__()    # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)     # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "# nn.ReLU是一个层结构（或者说是一个类），必须放在nn.Module中。F.relu是一个函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f645dffa",
   "metadata": {},
   "source": [
    "### 实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层，实现与之前一样的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f082aaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1426, -0.0279,  0.1653,  0.0338,  0.1751,  0.0290, -0.0180,  0.0950,\n",
       "         -0.2281,  0.1150],\n",
       "        [ 0.1585, -0.0684,  0.2853,  0.1438,  0.2383,  0.0324, -0.0236, -0.0057,\n",
       "         -0.1298,  0.1387]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f110da25",
   "metadata": {},
   "source": [
    "### 顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff17405e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0790,  0.2283,  0.1794, -0.3227, -0.2281, -0.1575,  0.1452, -0.1098,\n",
       "          0.0490, -0.4712],\n",
       "        [-0.1850,  0.1990,  0.0770, -0.3088, -0.1134, -0.3073,  0.1700, -0.1171,\n",
       "          0.0890, -0.3641]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):    # *args: list of input argument\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081da494",
   "metadata": {},
   "source": [
    "### 当sequential类不能满足需求时，可以在init和forward里自定义函数\n",
    "- 在正向传播函数中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1244018b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2289, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "    \n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254f34d",
   "metadata": {},
   "source": [
    "### 混合搭配各种组合块的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b4a2ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0698, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8022a71",
   "metadata": {},
   "source": [
    "### 总结\n",
    "- 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "- 块可以包含代码。\n",
    "- 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "- 层和块的顺序连接由Sequential块处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6424ea0",
   "metadata": {},
   "source": [
    "# 2. 访问参数（参数管理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e6a4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3345],\n",
       "        [-0.3280]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先关注具有单隐藏层的多层感知机\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)\n",
    "\n",
    "# 之所以没有初始化w，是因为在调用net()的时候，PyTroch自动初始化了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b45ad",
   "metadata": {},
   "source": [
    "### 访问每层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56f5dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.1482,  0.0290,  0.2475,  0.3481, -0.1811, -0.1143,  0.1727, -0.0954]])), ('bias', tensor([-0.3471]))])\n"
     ]
    }
   ],
   "source": [
    "# net[2]：返回nn.Sequential最后一层（线性层 nn.Linear(8, 1）\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f43649",
   "metadata": {},
   "source": [
    "### 直接访问目标参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "746b2f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.3471], requires_grad=True)\n",
      "tensor([-0.3471])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "# Parameter 定义一个可以优化的参数\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fde7f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1482,  0.0290,  0.2475,  0.3481, -0.1811, -0.1143,  0.1727, -0.0954]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9de1f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过grad访问梯度\n",
    "net[2].weight.grad == None\n",
    "# 因为没有做反向计算，所以grad=none。当backward后（训练后），grad就能访问元素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5805110",
   "metadata": {},
   "source": [
    "## 一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1886788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# *：解包元组\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])\n",
    "# 0是第1线性层，2是第二线性（全连接）层。1是relu层，拿不出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "012f494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', torch.Size([8, 4])), ('bias', torch.Size([8]))]\n",
      "[('0.weight', torch.Size([8, 4])), ('0.bias', torch.Size([8])), ('2.weight', torch.Size([1, 8])), ('2.bias', torch.Size([1]))]\n"
     ]
    }
   ],
   "source": [
    "print([(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print([(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af2365de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3471])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过名字获取参数\n",
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2917e1",
   "metadata": {},
   "source": [
    "### 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41197efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3055],\n",
       "        [-0.3052]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    # block2嵌套4个block1\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())    # f'block {i}'传一个字符串的名字(功能一样)，而非返回01234...\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8337c",
   "metadata": {},
   "source": [
    "## 显示设计的网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9afe8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7b8e1",
   "metadata": {},
   "source": [
    "### 因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。 下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8101ec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4108,  0.2007,  0.3326,  0.2777,  0.4136, -0.0349,  0.2626,  0.4421])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03bbb92",
   "metadata": {},
   "source": [
    "# 内置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8ed9b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0010,  0.0007,  0.0172, -0.0120]), tensor(0.))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)    # 下划线表示会读输入进行改变\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# apply方法对net所有的layers都进行内置初始化\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "995804b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)    # 把权重初始化成值为1\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466b9ac",
   "metadata": {},
   "source": [
    "## 对某些块应用不同的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b061e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4617,  0.2949, -0.5163, -0.5965])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):    # xavier初始化：每一层输出的方差应该尽量相等\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d39db",
   "metadata": {},
   "source": [
    "### 自定义初始化\n",
    "\n",
    "<img src=\"./pic/自定义初始化.PNG\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72151aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0534, -0.0000, -9.8683, -9.1118],\n",
       "        [-0.0000,  0.0000, -6.1340, -9.7358]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        # 保留绝对值大于5的权重，否则设成0\n",
    "        m.weight.data *= m.weight.data.abs() >= 5    # >=的优先级高于*=\n",
    "        # 这里*=的代码相当于先计算一个布尔矩阵，然后再用布尔矩阵的对应元素去乘以原始矩阵的每个元素\n",
    "\n",
    "        net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2497a43",
   "metadata": {},
   "source": [
    "### 可以直接设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75ef70bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000, -8.8683, -8.1118])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddad27",
   "metadata": {},
   "source": [
    "### 参数绑定（参数共享）\n",
    "- 有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae70aee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),    # 第0、6个隐藏层权重是自己的\n",
    "                    shared, nn.ReLU(),   # 第2、4个隐藏层share权重\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "\n",
    "net(X)\n",
    "\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "\n",
    "# 如果第二层权重改变，第4层也会一起改变。因为指向同一个parameter实例\n",
    "net[2].weight.data[0, 0] = 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cec873",
   "metadata": {},
   "source": [
    "# 3. 自定义层\n",
    "- 自定义层与自定义网络没区别，因为层也是`nn.module`的子类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331fc524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):    # 子类不定义init就会调用父类init\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "    \n",
    "\n",
    "layer = CenteredLayer()\n",
    "X = torch.FloatTensor([1, 2, 3, 4, 5])\n",
    "layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c264d189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(0.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean(), layer(X).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf13c27",
   "metadata": {},
   "source": [
    "## 将层作为组件合并到更复杂的模型中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4260a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.2596e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\n",
    "\n",
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()\n",
    "\n",
    "# 作为额外的健全性检查，我们可以在向该网络发送随机数据后，检查均值是否为0。\n",
    "# 由于处理的是浮点数，因为存储精度的原因，仍然可能会看到一个非常小的非零数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533b539",
   "metadata": {},
   "source": [
    "## 带参数的层\n",
    "\n",
    "- 调用参数类：`nn.Parameter`\n",
    "- 定义一个代参数（W,b）的自定义层\n",
    "    - 通过nn.Parameter包裹住初始化的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "419382e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0041,  0.0068, -1.1608],\n",
       "        [ 0.4268, -0.0303,  0.4747],\n",
       "        [ 0.4850,  0.3256, -0.6011],\n",
       "        [ 0.6265, -0.3294, -0.2187],\n",
       "        [ 0.3967,  1.1555, -1.7935]], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))  # 自动更新原权重\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "    \n",
    "\n",
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd74d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9828, 0.9877, 0.6760],\n",
       "         [0.1421, 0.6556, 0.1884]]),\n",
       " tensor([[-0.6655, -1.9394,  0.4634],\n",
       "         [-0.8074,  0.7427, -0.0662]]),\n",
       " tensor(0.6054),\n",
       " tensor(-0.3787))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.rand和torch.randn\n",
    "# rand是从0-1的均匀分布中随机抽样，randn是从0-1的正态分布中抽样，所以.mean()不为0\n",
    "p = torch.rand(2, 3)\n",
    "q = torch.randn(2, 3)\n",
    "p, q, p.mean(), q.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07921d60",
   "metadata": {},
   "source": [
    "### 使用自定义层直接执行前向传播计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dc41d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4066, 0.0000, 0.0000],\n",
       "        [0.7730, 0.9556, 0.0000]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f5a02",
   "metadata": {},
   "source": [
    "### 使用自定义层构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbb236af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.2110],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义实例放入sequential里，参与网络构造\n",
    "\n",
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc877a28",
   "metadata": {},
   "source": [
    "# 4. 读写文件\n",
    "- 保存`torch.save`和加载`torch.load`张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4753919c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')\n",
    "\n",
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41336ef8",
   "metadata": {},
   "source": [
    "## 存储一个张量列表，然后把它们读回内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f94e8945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ca2d0",
   "metadata": {},
   "source": [
    "## 写入或读取从字符串映射到张量的字典\n",
    "- 当要读取或写入模型中的所有权重时，这很方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd12c656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5069555",
   "metadata": {},
   "source": [
    "## 加载和保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29d53cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82ee6c",
   "metadata": {},
   "source": [
    "- 将模型的参数存储在一个叫做`mlp.params`的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "716b7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7df39",
   "metadata": {},
   "source": [
    "- 实例化了原始多层感知机模型的一个备份。 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "382d4382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()    # 先声明MLP（生成网络，参数已随机初始化）\n",
    "clone.load_state_dict(torch.load('mlp.params'))    # 通过load来覆盖随机初始化的权重\n",
    "clone.eval()    # 显示网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96ffa6",
   "metadata": {},
   "source": [
    "- 验证：\n",
    "    - 由于两个实例具有相同的模型参数，在输入相同的X时， 两个实例的计算结果应该相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7325f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f674122",
   "metadata": {},
   "source": [
    "train模式（net.train())和eval模式（net.eval())。\n",
    "一般的神经网络中，这两种模式是一样的，\n",
    "只有当模型中存在dropout和batchnorm的时候才有区别。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
