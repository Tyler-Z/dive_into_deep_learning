{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133e684f",
   "metadata": {},
   "source": [
    "# 感知机\n",
    "<img src=\"./pic/感知机.PNG\" width=300 height=300>\n",
    "\n",
    "## 训练感知机\n",
    "- 等价于批量大小为1的梯度下降\n",
    "<img src=\"./pic/训练感知机.PNG\" width=300 height=300>\n",
    "<img src=\"./pic/训练感知机1.PNG\" width=300 height=300>\n",
    "<img src=\"./pic/训练感知机2.PNG\" width=300 height=300>\n",
    "\n",
    "## 收敛定理\n",
    "- 什么时候停止训练\n",
    "<img src=\"./pic/训练感知机.PNG\" width=300 height=300>\n",
    "\n",
    "\n",
    "## XOR问题\n",
    "<img src=\"./pic/感知机XOR问题.PNG\" width=300 height=300>\n",
    "\n",
    "## 总结\n",
    "<img src=\"./pic/感知机总结.PNG\" width=300 height=300>\n",
    "\n",
    "# 多层感知机\n",
    "\n",
    "## 学习XOR\n",
    "- 先分类蓝色分类器，再分类橙色分类器，最后蓝与橙相乘得到灰色分类器\n",
    "<img src=\"./pic/学习XOR.PNG\" width=300 height=300>\n",
    "\n",
    "\n",
    "# 激活函数\n",
    "- 激活函数防止层数的塌陷：隐藏层之间不加激活函数的话，两个层就合并成一个线性层了\n",
    "\n",
    "## sigmoid激活函数\n",
    "<img src=\"./pic/sigmoid函数.PNG\" width=300 height=300>\n",
    "\n",
    "## Tanh激活函数\n",
    "<img src=\"./pic/Tanh激活函数.PNG\" width=300 height=300>\n",
    "\n",
    "## Relu激活函数\n",
    "- 我们不希望sigma(x)是一个线性函数，\n",
    "- 所以直接x>0, 导数为1；x < 0，导数为0\n",
    "- 主要好处：不需要指数运算，计算速度快（在cpu上，一次指数运算相当于100次乘法运算）\n",
    "- 可以防止梯度爆炸或者梯度消失\n",
    "<img src=\"./pic/Relu激活函数.PNG\" width=300 height=300>\n",
    "\n",
    "# 单隐藏层 - 单分类\n",
    "- 输入为n维向量\n",
    "- **注意：向量可以看作只有一列的矩阵，向量的转置可以看作是只有一行的矩阵**\n",
    "- 隐藏层大小为m，则权重是一个m*n的矩阵；偏移等于隐藏层大小，一个长为m的向量。\n",
    "- 隐藏层大小是超参数\n",
    "<img src=\"./pic/单隐藏层.PNG\" width=300 height=300>\n",
    "- 注：上图与下图无关，下图对应1个output\n",
    "<img src=\"./pic/单隐藏层 - 单分类.PNG\" width=300 height=300>\n",
    "- h是一个长为m的向量，作为输入层传到输出层o\n",
    "- 输出层的m可以理解成：前隐藏层大小m乘以输出个数，这里是单分类问题，输出个数为1，所以就是m x 1 = m\n",
    "- 之所以w2要转置，是因为w2为m维向量，shape(m, 1)，但要与h的shape(m, 1) 相乘，所以转置。h的shape是隐藏层w1决定的\n",
    "- 激活函数一定要是**非线性函数**，不然还是会有感知机问题（XOR问题）\n",
    "\n",
    "\n",
    "# 多类分类\n",
    "- 不加hidden layer，就是softmax回归\n",
    "- 加了hidden layer，就是多层感知机\n",
    "<img src=\"./pic/多类分类.PNG\" width=300 height=300>\n",
    "- k个输出单元，权重则是一个（m, k）的矩阵，便宜bias为长度为k的向量\n",
    "- Q：为什么对w2转置\n",
    "- A：因为要匹配矩阵与向量的大小 使他们能够相乘\n",
    "<img src=\"./pic/多类分类2.PNG\" width=300 height=300>\n",
    "- w2从向量变成了矩阵，b2从标量变成了向量\n",
    "- 对于output，做一次softmax\n",
    "\n",
    "# 多隐藏层\n",
    "- 对于每一个隐藏层，都有对应的w和b\n",
    "- 前一个隐藏层的输出，作为后一个隐藏层的输入\n",
    "- 输出层不需要激活函数\n",
    "- m1到m3再到output，每层大小逐步压缩\n",
    "- 例如：输入128维，m1隐藏层64维，m2隐藏层32维，m3隐藏层16维，output输出5类\n",
    "<img src=\"./pic/多隐藏层.PNG\" width=500 height=500>\n",
    "\n",
    "# 总结\n",
    "<img src=\"./pic/多类分类总结.PNG\" width=500 height=500>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
