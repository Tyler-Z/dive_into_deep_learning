{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8c03bc",
   "metadata": {},
   "source": [
    "# 标量导数\n",
    "\n",
    "<img src=\"./pic/标量导数.PNG\"  width=400  heigth=400>\n",
    "\n",
    "# 向量导数\n",
    "\n",
    "<img src=\"./pic/导数-向量.PNG\"  width=400  heigth=400>\n",
    "<img src=\"./pic/导数-向量-样例.PNG\"  width=400  heigth=400>\n",
    "\n",
    "## 导数是切线的斜率\n",
    "<img src=\"./pic/导数是切线的斜率.PNG\"  width=200  heigth=200>\n",
    "\n",
    "## 亚导数（sub gradient）\n",
    "- 当 x = 0时，有多个斜率\n",
    "- 不是偏导。多元函数才有偏导\n",
    "<img src=\"./pic/亚导数.PNG\"  width=300  heigth=300>\n",
    "<img src=\"./pic/亚导数-另一个例子.PNG\"  width=300  heigth=300>\n",
    "\n",
    "\n",
    "# 梯度\n",
    "- 将导数拓展到向量\n",
    "- 梯度指向 值变化最大的方向\n",
    "\n",
    "<img src=\"./pic/梯度-将导数拓展到向量.PNG\"  width=300  heigth=300>\n",
    "\n",
    "\n",
    "\n",
    "|-|标量x|向量X|\n",
    "|-|-|-|\n",
    "|标量y|标量|行向量|\n",
    "|标量Y|列向量|矩阵|\n",
    "\n",
    "- 解释1：https://zhuanlan.zhihu.com/p/94746630\n",
    "- **自变量被锁定为列向量**，不要有任何花里胡哨的想法。\n",
    "- 分子布局 vs 分母布局\n",
    "    - 上图为分子布局\n",
    "    - https://blog.csdn.net/a6822342/article/details/86299573\n",
    "\n",
    "- <img src=\"./pic/梯度-例子.PNG\"  width=300  heigth=300>\n",
    "- 梯度与等高线是正交的，梯度指向的值（2,4）是变化最大的方向\n",
    "    - 等高线指的是地形图上高度相等的相邻各点所连成的闭合曲线\n",
    "    - 山谷线和山脊线与等高线正交\n",
    "    - **负梯度（梯度下降）就是山谷线或山脊线**\n",
    "\n",
    "- CIFAR-10数据集上VGG网络的损失曲面\n",
    "<img src=\"./pic/CIFAR-10数据集上VGG网络的损失曲面.jpg\"  width=300  heigth=300>\n",
    "\n",
    "\n",
    "## 标量y对列向量x的微分（求导）\n",
    "\n",
    "- 标量y对列向量x的求导，实际上是**求对x中每个元素的偏导数**\n",
    "    - 但是这里需要注意，y为1*1的标量，若x为n*1的向量，则可以假设y=F(x)=Ax, y+dy=F(x)+dy/dx * dx. 显然A为1*n的向量，dy/dx 为1*n的向量，因此标量y对列向量x的求导为1行m列的列向量，和x'同维度，每个元素对应表示y对x中对应元素的偏导数。\n",
    "<img src=\"./pic/标量y对列向量x的微分.PNG\"  width=300  heigth=300>\n",
    "\n",
    "- 解释2：https://zhuanlan.zhihu.com/p/427021103\n",
    "    - 采用标量对列向量的微分（求导）为基本表达方式\n",
    "        - 在实际使用中，由于变量经常表达为列向量形式\n",
    "        - 向量和矩阵的乘法不满足交换律，假设存在1/dx，则仅有第对列向量求导可以得到dy/dx = A ，对行向量的求导得到的是 1/dx * dy = A\n",
    "        \n",
    "        \n",
    "##  列向量y对列向量x的求导\n",
    "\n",
    "这里有两个理解思路:\n",
    "\n",
    "- 列向量y为一系列标量按行堆叠，则其导数也应当为一系列标量对矢量求导结果按行堆叠。恰好标量对列向量求导结果为行向量，适合按行堆叠。\n",
    "- y为m * 1的向量，x为n * 1的向量，假设y=F(x)=Ax, y+dy=x+dy/dx * dx. 显然A为m * n的矩阵，dy/dx 为m * n的矩阵\n",
    "\n",
    "<img src=\"./pic/列向量y对列向量x的求导.PNG\"  width=300  heigth=300>\n",
    "\n",
    "\n",
    "# 拓展到矩阵\n",
    "<img src=\"./pic/导数-拓展到矩阵.PNG\"  width=500  heigth=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea33a123",
   "metadata": {},
   "source": [
    "# 向量链式法则\n",
    "\n",
    "<img src=\"./pic/向量链式法则.PNG\"  width=500  heigth=500>\n",
    "\n",
    "### 例子1\n",
    "<img src=\"./pic/向量链式法则-例子.PNG\"  width=500  heigth=500>\n",
    "\n",
    "### 例子2\n",
    "<img src=\"./pic/向量链式法则-例子2.PNG\"  width=500  heigth=500>\n",
    "\n",
    "\n",
    "# 自动求导\n",
    "<img src=\"./pic/自动求导.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 计算图\n",
    "- 怎样做出自动求导的？等价于用链式法则求导的过程\n",
    "<img src=\"./pic/计算图.PNG\"  width=500  heigth=500>\n",
    "- 显示构造（使用不方便）\n",
    "    - Tensorflow / Theano / MXNet\n",
    "<img src=\"./pic/显示构造.PNG\"  width=500  heigth=500>\n",
    "- 隐式构造\n",
    "    - PyTorch / MXNet\n",
    "<img src=\"./pic/隐式构造.PNG\"  width=500  heigth=500>\n",
    "\n",
    "\n",
    "# 自动求导的两种模式\n",
    "- 正向积累（从右往左）\n",
    "- 反向积累（从左往右）**backpropagation**\n",
    "<img src=\"./pic/自动求导的两种模式.PNG\"  width=500  heigth=500>\n",
    "- 先正向积累计算出y，再反向积累求导\n",
    "\n",
    "## 反向积累 backpropagation\n",
    "- step0:\n",
    "<img src=\"./pic/反向积累bp.PNG\"  width=500  heigth=500>\n",
    "- step1:\n",
    "    - b是之前计算的结果，所以需要先保存，再读取\n",
    "<img src=\"./pic/反向积累bp2.PNG\"  width=500  heigth=500>\n",
    "- step2:\n",
    "    - 计算z对于a的导数\n",
    "<img src=\"./pic/反向积累bp3.PNG\"  width=500  heigth=500>\n",
    "- step3:\n",
    "    - 计算z对于w的导数：既需要w的值，也需要dz/da的值\n",
    "<img src=\"./pic/反向积累bp4.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 反向积累总结\n",
    "- 反向执行，如果遇到不需要的点，则无需计算（图中的灰色点，不计算）。但需要使用前向积累的中间结果\n",
    "<img src=\"./pic/反向积累总结.PNG\"  width=500  heigth=500>\n",
    "- 反向积累复杂度\n",
    "    - 需要GPU的祸源：需要存储正向的所有中间结果\n",
    "<img src=\"./pic/反向积累复杂度.PNG\"  width=500  heigth=500>\n",
    "\n",
    "- 为什么用反向，而不用正向：\n",
    "    - 反向从根节点向下扫，可以保证每个节点只扫一次；正向从叶节点向上扫，会导致上层节点可能需要被重复扫多次（正向中，子节点比父节点先计算，因此也无法像反向那样把本节点的计算结果传给每一个子节点）\n",
    "    - 每个叶子节点看成变量，正向累积计算每个叶子节点都需要扫一遍计算图，反向只需要扫一遍（因为中间节点存储了反向的导数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a90005",
   "metadata": {},
   "source": [
    "# 自动求导\n",
    "\n",
    "- 对函数y = 2x^T * x，关于列向量x求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9e9911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b685caf",
   "metadata": {},
   "source": [
    "## 1. 在计算【y】关于【x】的梯度之前，我们需要一个地方来存储梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d751009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)    # 等价于 x = torch.arange(4.0, requires_grad=True)\n",
    "x.grad    # 默认值是None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f54a47",
   "metadata": {},
   "source": [
    "## 2. 计算【y】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483d8288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 内积就是点乘：对应元素相乘再相加\n",
    "y = 2 * torch.dot(x, x)    # （dot函数自动转置）1*1 + 2*2 + 3*3 = 14，y = 14*2 = 28\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbccd3",
   "metadata": {},
   "source": [
    "## 3. 通过调用反向传播函数来自动计算【y】关于【x】每个分量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa2e6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()    # 求导\n",
    "x.grad    # 访问所有的导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc8f3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = (x * x的内积) * 2 ==> y' = 4*x\n",
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e21d8c",
   "metadata": {},
   "source": [
    "## 4. 计算【x】的另一个函数\n",
    "\n",
    "### 在默认情况下，PyTorch会累积梯度，需要清除之前的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb3b3cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()    # 梯度清空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70adbcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07f80af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.sum()    # y = 6\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39bb8e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c067d",
   "metadata": {},
   "source": [
    "## 5. 深度学习的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和\n",
    "\n",
    "### Q:  为什么要 y.sum() 之后，再反向积累\n",
    "### A:  标量关于向量求导 得到的是一个向量；那向量关于向量求导 得到的就是一个矩阵；这里就是把向量通过sum的形式转化成了标量，进行求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9bd1864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>),\n",
       " tensor(14., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用 'backward' 需要传入一个 'gradient' 参数，该参数指定微分函数\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y, y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37227d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y.backward()    # Error: grad can be implicitly created only for scalar outputs。求和就是为了变成标量，不是标量没法输出梯度\n",
    "# Loss是标量。如果是向量，则向量对向量【x】求导是矩阵，对矩阵【x】求导是4维矩阵，过于庞大的张量\n",
    "y.sum().backward()    # 等价于 y.backward(torch.ones(len(x)))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a737ffb",
   "metadata": {},
   "source": [
    "## 6. 将某些计算移动到记录的计算图之外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45edfd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()    # 把y当作一个常数，而非关于x的函数。所以u不是一个关于x的函数，而是一个常数，值为x^2\n",
    "# detach函数能把【y】退化成与【x】无关的量\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27bcff05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 但y仍然是一个关于x的函数，任可以通过对【y】求和，再对【x】求导\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14d749",
   "metadata": {},
   "source": [
    "## 7.即使构建函数的计算图需要通过Python控制流（例如，条件、循环、或任意函数调用），我们仍然可以计算得到的变量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23524be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用f(a)函数返回的值，是一个关于a的线性函数，所以d关于a的导数就是这个线性函数中a的系数\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "倒着看，d=f(a)=c,而c要么是b要么是b*100，而b要么是a*2，要么是a乘以好多个2，综上得到d最终结果都是a乘以几个实数，\n",
    "那这个实数用A表示，则整体上d=f(a)=Aa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
