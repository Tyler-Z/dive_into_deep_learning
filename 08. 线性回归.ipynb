{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d8b5d6",
   "metadata": {},
   "source": [
    "## 一个简化模型\n",
    "<img src=\"./pic/线性回归-假设.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 线性模型\n",
    "<img src=\"./pic/线性模型.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 线性模型可以看作是单层神经网络\n",
    "<img src=\"./pic/线性模型-单层神经网络.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 衡量预估质量\n",
    "- 1/2 是为了求导的时候，把指数2消除\n",
    "<img src=\"./pic/衡量预估质量.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 训练数据\n",
    "<img src=\"./pic/训练数据.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 参数学习\n",
    "- 对于模型，在每一个数据上的损失，求均值，得到损失函数\n",
    "- ||y - Xw - b||<sup>2</sup> 对向量求L2 norm\n",
    "- 目标就是找到【w】和【b】使损失函数最小\n",
    "<img src=\"./pic/参数学习.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 显示解\n",
    "- (X<sup>T</sup> X)<sup>-1</sup> 是矩阵(X<sup>T</sup> X)的逆矩阵\n",
    "<img src=\"./pic/显示解.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 总结\n",
    "<img src=\"./pic/线性回归-总结.PNG\"  width=500  heigth=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b293f6",
   "metadata": {},
   "source": [
    "## 基础优化方法\n",
    "<img src=\"./pic/基础优化方法.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 梯度下降\n",
    "- 当一个模型没有显示解时，使用梯度下降(常用)\n",
    "- 初始一个w<sub>0</sub>，不断更新w<sub>0</sub>使得有最优解\n",
    "- 标量loss对向量w<sub>t-1</sub>求导，得到梯度\n",
    "- 梯度为函数的值，增加最快的方向；负梯度为下降最快的方向，就是黄线的方向\n",
    "- yeta为学习率（标量），表示每一次沿梯度下降的方向走多远\n",
    "<img src=\"./pic/梯度下降.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 选择学习率\n",
    "- 计算梯度是训练模型最贵的一部分，不易设置太小\n",
    "- 不易太大，一直在震荡，不一定会下降\n",
    "<img src=\"./pic/选择学习率.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 深度学习中，很少直接使用梯度下降。最常使用的是小批量随机梯度下降（SGD）\n",
    "- 梯度下降里计算梯度，要对整个损失函数求导，而损失函数是对所有样本的平均损失。在计算梯度时，会把所有样本计算一遍，很贵\n",
    "<img src=\"./pic/小批量随机梯度下降.PNG\"  width=500  heigth=500>\n",
    "- 解决办法：用b个样本来近似整个样本集上的平均损失\n",
    "    - b很大时，近似精确\n",
    "    - b很小时，不那么精确\n",
    "    - 计算梯度的复杂度，与样本个数线性相关\n",
    "    \n",
    "## 选择批量大小\n",
    "<img src=\"./pic/选择批量大小.PNG\"  width=500  heigth=500>\n",
    "\n",
    "## 每次批量计算梯度时，要把梯度清0\n",
    "- 因为PyTorch不会自动把梯度清0\n",
    "\n",
    "## 总结\n",
    "<img src=\"./pic/梯度下降总结.PNG\"  width=500  heigth=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d58364",
   "metadata": {},
   "source": [
    "- 对于一个大小为N的训练集，如果每个epoch中mini-batch的采样方法采用最常规的N个样本每个都采样一次，设mini-batch大小为b，那么每个epoch所需的迭代次数(正向+反向)为 N/b , 因此完成每个epoch所需的时间大致也随着迭代次数的增加而增加。\n",
    "\n",
    "- 由于目前主流深度学习框架处理mini-batch的反向传播时，默认都是先将每个mini-batch中每个instance得到的loss平均化之后再反求梯度，也就是说每次反向传播的梯度是对mini-batch中每个instance的梯度平均之后的结果，所以**b的大小决定了相邻迭代之间的梯度平滑程度，b太小，相邻mini-batch间的差异相对过大，那么相邻两次迭代的梯度震荡情况会比较严重，不利于收敛；b越大，相邻mini-batch间的差异相对越小，虽然梯度震荡情况会比较小，一定程度上利于模型收敛，但如果b极端大，相邻mini-batch间的差异过小，相邻两个mini-batch的梯度没有区别了，整个训练过程就是沿着一个方向蹭蹭蹭往下走，很容易陷入到局部最小值出不来。**\n",
    "\n",
    "- 总结下来：batch size过小，花费时间多，同时梯度震荡严重，不利于收敛；batch size过大，不同batch的梯度方向没有任何变化，容易陷入局部极小值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4ccd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
