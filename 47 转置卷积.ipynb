{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b60202b2",
   "metadata": {},
   "source": [
    "# 转置卷积\n",
    "\n",
    "- **转置卷积不是卷积的逆运算**\n",
    "    - 只是将特征图的大小，还原为卷积之前的特征图大小，但**数值与之前的特征图不同**\n",
    "- **转置卷积也是一种卷积**\n",
    "    - 他将输入和核进行了**重新排列**\n",
    "    - 同卷积一般是做下采样不同，它通常用作**上采样**\n",
    "    - 如果卷积将输入从$(h, w)$变成了$(h', w')$，同样超参数下它将$(h', w')$变成了$(h, w)$\n",
    "- 卷积不会增大高宽：要么不变，要么变小\n",
    "- 转置卷积的用处：\n",
    "    - 在语义分割中，卷积不断减少高宽，如何做像素级别的输出。所以需要转置卷积把小的高宽变大\n",
    "- 转置卷积增大输出图片的高宽\n",
    "    - stride越大，输出的高宽也越大\n",
    "\n",
    "    <img src=\"./pic/转置卷积作用.PNG\" width=400 height=400>\n",
    "- 计算：\n",
    "<img src=\"./pic/转置卷积.PNG\" width=400 height=400>\n",
    "<img src=\"./pic/转置卷积计算.PNG\" width=400 height=400>\n",
    "\n",
    "## 转置卷积运算步骤\n",
    "\n",
    "1. 在输入特征图元素间填充s-1行、列0\n",
    "2. 在输入特征图四周填充k-p-1行、列0\n",
    "3. 将卷积核参数上下、左右翻转\n",
    "4. 做正常卷积运算（填充0，步距1）\n",
    "\n",
    "<table><tr>\n",
    "    <td><img src=\"./pic/转置卷积s=1, p=0, k=3.gif\" width=200 height=200>s=1, p=0, k=3</td>\n",
    "    <td><img src=\"./pic/转置卷积s=2, p=0, k=3.gif\" width=200 height=200>s=1, p=0, k=3</td>\n",
    "    <td><img src=\"./pic/转置卷积s=2, p=1, k=3.gif\" width=200 height=200>s=1, p=0, k=3</td>\n",
    "</tr></table>\n",
    "\n",
    "- 公式：\n",
    "    - $H_{out} = (H_{in} - 1) \\times stride - 2 \\times padding + kernel Size$\n",
    "    - $W_{out} = (W_{in} - 1) \\times stride - 2 \\times padding + kernel Size$\n",
    "\n",
    "## 转置卷积运算例子（变成卷积的输入与核，再做卷积运行）：\n",
    "- **最后的卷积操作中（填充0，步幅1），与转置卷积中的填充与步幅无关**\n",
    "\n",
    "- 例1：高宽 = $2 \\times 2$, k = 2, p = 0, s = 1\n",
    "    - 输出为：(2 - 1) x 1 - 2 x 0 + 2 = 3\n",
    "<img src=\"./pic/转置卷积运算例子1.PNG\" width=400 height=400>\n",
    "\n",
    "- 例2：高宽 = $2 \\times 2$, k = 2, p = 1, s = 1\n",
    "    - 输出为：(2 - 1) x 1 - 2 x 1 + 2 = 1\n",
    "<img src=\"./pic/转置卷积运算例子2.PNG\" width=400 height=400>\n",
    "\n",
    "- 例3：高宽 = $2 \\times 2$, k = 2, p = 0, s = 2\n",
    "    - 输出为：(2 - 1) x 2 - 2 x 0 + 2 = 4\n",
    "<img src=\"./pic/转置卷积运算例子3.PNG\" width=400 height=400>\n",
    "<img src=\"./pic/转置卷积运算例子3手写.jpg\" width=400 height=400>\n",
    "\n",
    "## 形状换算\n",
    "- 如果$(n-k-2p+s)$刚好可以整除$s$，则$n=sn'+k-2p-s$，否则大于\n",
    "<img src=\"./pic/转置卷积形状换算1.PNG\" width=400 height=400>\n",
    "\n",
    "## 为什么叫转置\n",
    "\n",
    "- 卷积：$Y'(n \\times 1) = V(n \\times m) \\odot X'(m \\times 1)$\n",
    "- 转置：$Y'(m \\times 1) = V(m \\times n) \\odot X'(n \\times 1)$\n",
    "- 这里的向量版本应该指将矩阵flatten成列向量\n",
    "<img src=\"./pic/为什么叫转置.PNG\" width=400 height=400>\n",
    "\n",
    "\n",
    "## 同反卷积的关系\n",
    "<img src=\"./pic/同反卷积的关系.PNG\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80259401",
   "metadata": {},
   "source": [
    "# 转置卷积的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd42c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1adf2",
   "metadata": {},
   "source": [
    "## 实现基本的转置卷积运算\n",
    "\n",
    "- 无padding，stride=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371406b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_conv(X, K):\n",
    "    h, w = K.shape\n",
    "    # 卷积高宽计算公式（图片h - 核h + 1）\n",
    "    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Y[i: i + h, j: j + w] += X[i, j] * K\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b38c71",
   "metadata": {},
   "source": [
    "## 验证上述实现输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8dee7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  1.],\n",
       "        [ 0.,  4.,  6.],\n",
       "        [ 4., 12.,  9.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "trans_conv(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55233dd8",
   "metadata": {},
   "source": [
    "## 使用高级API获得相同的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7d40c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  1.],\n",
       "          [ 0.,  4.,  6.],\n",
       "          [ 4., 12.,  9.]]]], grad_fn=<SlowConvTranspose2DBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1：批量大小\n",
    "# 1：通道数\n",
    "X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\n",
    "# 1：输入通道数\n",
    "# 1：输出通道数\n",
    "# 不需要偏差\n",
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef0d6c",
   "metadata": {},
   "source": [
    "# 填充、步幅和多通道\n",
    "\n",
    "- 卷积高宽计算公式：\n",
    "    - 高：$(图h - 核h + padding + 步长) \\div 步长$\n",
    "    - 宽：$(图w - 核w + padding + 步长) \\div 步长$\n",
    "- 转置卷积高宽计算公式：\n",
    "    - $H_{out} = (H_{in} - 1) \\times stride - 2 \\times padding + kernel Size$\n",
    "    - $W_{out} = (W_{in} - 1) \\times stride - 2 \\times padding + kernel Size$\n",
    "    \n",
    "- 与常规卷积不同，在转置卷积中，填充被应用于的输出（常规卷积将填充应用于输入）\n",
    "    - 例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列。\n",
    "    - Padding是指上采样之后的图对应于原图是否扩充了\n",
    "- 多通道：与卷积相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f539bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[4.]]]], grad_fn=<SlowConvTranspose2DBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)\n",
    "\n",
    "# 原tconv(X)是一个3x3矩阵\n",
    "# padding=1后\n",
    "# 成单个元素（输出的大小变小了）\n",
    "# 填充在输出上，本来2*2与2*2转置卷积得到3*3，填充在输出上，就是填充在3*3上，得到的结果就成了一个数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcde1075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 1.],\n",
       "          [0., 0., 2., 3.],\n",
       "          [0., 2., 0., 3.],\n",
       "          [4., 6., 6., 9.]]]], grad_fn=<SlowConvTranspose2DBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# （图高+核高-步长）* 步长=（2 + 2 - 2）* 2 = 4\n",
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee88f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转置卷积主要是作用在单通道上面。对于多通道，与卷积相同\n",
    "\n",
    "X = torch.rand(size=(1, 10, 16, 16))\n",
    "conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)    # w =（16-5+2*2+3）/3= 6 \n",
    "tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)    # w = (6 - 1)*3 - 2*2 + 5 = 16 \n",
    "tconv(conv(X)).shape == X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef414b40",
   "metadata": {},
   "source": [
    "### 将特征图的大小，还原为卷积之前的特征图大小，但数值与之前的特征图不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6965f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv(conv(X)).sum == X.sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08ac54",
   "metadata": {},
   "source": [
    "## 与矩阵变换的联系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64577995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27., 37.],\n",
       "        [57., 67.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(9.0).reshape(3, 3)\n",
    "K = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "Y = d2l.corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d1a7b",
   "metadata": {},
   "source": [
    "- 给一个kernel，变成上文的V。变成一个大矩阵，使得卷积变成矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80959c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 0., 3., 4., 0., 0., 0., 0.],\n",
       "        [0., 1., 2., 0., 3., 4., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 2., 0., 3., 4., 0.],\n",
       "        [0., 0., 0., 0., 1., 2., 0., 3., 4.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 就是将卷积的运算变为了矩阵和向量的乘法，笨办法手推一下就好\n",
    "def kernel2matrix(K):\n",
    "    # 之所以W是4x9，是因为输入是3x3，拉长是长为9的向量（1行9列）。W*X，所以W的列数是9\n",
    "    # 因为输出是2x2，所以要做4次卷积操作，所以有4行，每行对应输出元素的卷积\n",
    "    k, W = torch.zeros(5), torch.zeros((4, 9))\n",
    "    k[:2], k[3:5] = K[0, :], K[1, :]\n",
    "    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k\n",
    "    return W\n",
    "\n",
    "W = kernel2matrix(K)\n",
    "W\n",
    "# [1., 2., 0., 3., 4., 0., 0., 0., 0.]：卷积核k，对第1块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d027974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cbe6fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y是用卷积做出来的Y\n",
    "Y == torch.matmul(W, X.reshape(-1)).reshape(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d55e4bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = trans_conv(Y, K)\n",
    "Z == torch.matmul(W.T, Y.reshape(-1)).reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c821f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "转置而不是逆运算，所以形状是变回去了，但数值不会变回X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
